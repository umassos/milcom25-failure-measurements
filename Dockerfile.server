# Dockerfile for Jetson Orin Nano - S1 Server
# Based on NVIDIA L4T (Linux for Tegra) runtime

FROM nvcr.io/nvidia/l4t-pytorch:r35.2.1-pth2.0-py3
LABEL authors="Krishna Praneet Gudipaty"
LABEL service="inference-server"
LABEL description="Inference server for Jetson"

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    libgomp1 \
    libgcc-s1 \
    libstdc++6 \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN python3 -m pip install --upgrade pip

# Install ONNX Runtime for Jetson
# Note: Using CPU version as base, CUDA support will be enabled via environment
RUN pip3 install onnxruntime==1.16.3

# Install other Python dependencies
RUN pip3 install \
    grpcio==1.59.3 \
    grpcio-tools==1.59.3 \
    protobuf==4.25.1 \
    thop \
    pandas

RUN python3 --version
# Create app directory
WORKDIR /app

# Declare models directory as a volume for mounting
VOLUME ["/app/onnx_models"]

# Copy requirements (for reference, but we install specific versions above)
COPY requirements.txt ./

# Expose the default port
EXPOSE 8180

# # Set the default command
# ENTRYPOINT ["python3", "system/single_server.py"]

# # Default arguments for S1 server
# CMD ["-m", "ensemble-effnet-c5-lr-0.005-tin", "-n", "1", "-p", "8180", "-s", "localhost:8185"]
